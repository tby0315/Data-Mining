{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d6148a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'choice', 'string', 'number', 'point', 'helppct-up', 'recent8help', 'asymptoteA-up', 'asymptoteB-up']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(\"ca1-dataset.csv\")\n",
    "df2 = pd.read_csv(\"ca2-dataset.csv\")\n",
    "\n",
    "#find no variance data in ca2\n",
    "cols_with_zero = [col for col in df2.columns if df2[col].nunique() == 1]\n",
    "\n",
    "print(cols_with_zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30184b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avghelp', 'Avgchoice', 'Avgstring', 'Avgnumber', 'Avgpoint', 'Avghelppct-up', 'Avgrecent8help', 'AvgasymptoteA-up', 'AvgasymptoteB-up']\n"
     ]
    }
   ],
   "source": [
    "#find no variance data in ca1\n",
    "cols_with_zero_1 = [col for col in df1.columns if df1[col].nunique() == 1]\n",
    "\n",
    "print(cols_with_zero_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc4d418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pknow-1   Pknow-2   pchange\n",
      "pknow-1  1.000000  0.424843 -0.981923\n",
      "Pknow-2  0.424843  1.000000 -0.305216\n",
      "pchange -0.981923 -0.305216  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Selecting the  Pknow-1 &2 & Pchange and calculating correlation\n",
    "selected_cols = df2[['pknow-1','Pknow-2','pchange']]\n",
    "correlation_matrix = selected_cols.corr()\n",
    "\n",
    "print(correlation_matrix)\n",
    "\n",
    "# print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b06b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 howmanywrong-up  wrongpct-up\n",
      "howmanywrong-up         1.000000     0.553766\n",
      "wrongpct-up             0.553766     1.000000\n"
     ]
    }
   ],
   "source": [
    "# Selecting the  howmanywrong-up & wrongpct-up and calculating correlation\n",
    "selected_cols = df2[['howmanywrong-up','wrongpct-up']]\n",
    "correlation = selected_cols.corr()\n",
    "\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5358cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   timeSDnormed  timelast3SDnormed  timelast5SDnormed\n",
      "timeSDnormed           1.000000           0.096334           0.107049\n",
      "timelast3SDnormed      0.096334           1.000000           0.855967\n",
      "timelast5SDnormed      0.107049           0.855967           1.000000\n"
     ]
    }
   ],
   "source": [
    "# Selecting the  timeSDnormed','timelast3SDnormed','timelast5SDnormed and calculating correlation\n",
    "selected_cols = df2[['timeSDnormed','timelast3SDnormed','timelast5SDnormed']]\n",
    "correlation = selected_cols.corr()\n",
    "\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cc9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Prev3Count-up  Prev5Count-up\n",
      "Prev3Count-up       1.000000       0.956364\n",
      "Prev5Count-up       0.956364       1.000000\n"
     ]
    }
   ],
   "source": [
    "# Selecting the  'Prev3Count-up'&'Prev5Count-up'and calculating correlation\n",
    "selected_cols = df2[['Prev3Count-up','Prev5Count-up']]\n",
    "correlation = selected_cols.corr()\n",
    "\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c74a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Dummy categorical columns\n",
    "dummies = pd.get_dummies(df2[['prod', 'cell','lesson']], prefix=['dummy_prod', 'dummy_cell','dummy_lesson'])\n",
    "# Add the 'ID' column to the dummy DataFrame for merging\n",
    "dummies['Unique-id'] = df2['Unique-id']\n",
    "\n",
    "# Merging 'pknow-1'& 'Pknow-2' into ca1 using Unique-id\n",
    "merged_p = pd.merge(df1,df2[['Unique-id', 'pknow-1', 'Pknow-2','namea']], on='Unique-id', how='left')\n",
    "\n",
    "# Merging 'dummy_prod', 'dummy_cell' into ca1 using Unique-id\n",
    "merged1 = pd.merge(merged_p,dummies, on='Unique-id', how='left')\n",
    "\n",
    "\n",
    "# print(merged_dummy)\n",
    "# print(ca1_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bef756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature5: Calculate the total number of steps\n",
    "df2['totalstep'] = np.where(df2['wrongpct-up'] == 0, \n",
    "                            0, \n",
    "                            (df2['howmanywrong-up'] / df2['wrongpct-up']))\n",
    "merged5 = pd.merge(merged1,df2[['totalstep','Unique-id']], on='Unique-id', how='left')\n",
    "\n",
    "# Feature6: Calculate The total number of actions\n",
    "df2['totalaction'] = np.where(df2['timeperact-up'] == 0, \n",
    "                            0, \n",
    "                            (df2['time'] / df2['timeperact-up']))\n",
    "# merged6 = pd.merge(merged5,df2[['totalaction','Unique-id']], on='Unique-id', how='left')\n",
    "\n",
    "# Feature7\n",
    "df2['last5'] = df2['Prev5Count-up']*df2['timelast5SDnormed']\n",
    "# merged7 = pd.merge(merged6,df2[['last5','Unique-id']], on='Unique-id', how='left')\n",
    "\n",
    "# Feature8: if the wrong productions in the last five actions occupied a large percentage. \n",
    "df2['wrongper'] = np.where(df2['howmanywrong-up'] == 0, \n",
    "                            0, \n",
    "                            (df2['recent5wrong'] / df2['howmanywrong-up']))\n",
    "# if 'recent5wrong' in df2.columns:\n",
    "#     df2['wrongper'] = np.where(df2['howmanywrong-up'] == 0, \n",
    "#                                 0, \n",
    "#                                 (df2[' recent5wrong'] / df2['howmanywrong-up']))\n",
    "# else:\n",
    "#     print(\"Column 'recent5wrong' does not exist!\")\n",
    "# merged8 = pd.merge(merged7,df2[['wrongper','Unique-id']], on='Unique-id', how='left')\n",
    "\n",
    "# Feature9: Calculate percentage of actions where the production was wrong in the overall number of notright\n",
    "df2['numerator'] = df2['howmanywrong-up']*merged5['totalstep']\n",
    "df2['percent_wrong'] = np.where(df2['notright'] == 0, \n",
    "                            0, \n",
    "                            (df2['numerator'] / df2['notright']))\n",
    "# merged9 = pd.merge(merged8,df2[['percent_wrong','Unique-id']], on='Unique-id', how='left')\n",
    "\n",
    "# Feature10: Calculate The total number of actions\n",
    "df2['ratio5'] = np.where(df2['Prev5Count-up'] == 0, \n",
    "                            0, \n",
    "                            (df2['recent5wrong'] / df2['Prev5Count-up']))\n",
    "# ca1_merged = pd.merge(merged9,df2[['ratio5','Unique-id']], on='Unique-id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Merge all at once\n",
    "all_features = ['totalstep', 'totalaction', 'last5', 'wrongper', 'percent_wrong', 'ratio5', 'Unique-id']\n",
    "merged_all = pd.merge(merged1, df2[all_features], on='Unique-id', how='left')\n",
    "\n",
    "# print(df2.columns)\n",
    "\n",
    "# print(merged_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f92c81af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for new Dataset: 0.3450852848009799\n",
      "Cohen's Kappa for CA1 Dataset: 0.2967050796364549\n",
      "ROC AUC for new Dataset: 0.6370896898042206\n",
      "ROC AUC for CA1 Dataset: 0.6267461461873862\n",
      "new Dataset has a higher Cohen's Kappa score.\n",
      "new Dataset has a higher ROC AUC score.\n"
     ]
    }
   ],
   "source": [
    "#CA2\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    cohen_kappa_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "\n",
    "def evaluate_dataset(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "        # Create a dictionary to store group information\n",
    "    group_dict = {}\n",
    "    groups = np.array([])\n",
    "\n",
    "    # Iterate through the dataset to create groups\n",
    "    for index, row in dataset.iterrows():\n",
    "        namea = row['namea']\n",
    "        if namea not in group_dict:\n",
    "            group_dict[namea] = index\n",
    "        groups = np.append(groups, group_dict[namea])\n",
    "\n",
    "    # Initialize the GroupKFold cross-validator\n",
    "    group_kfold = GroupKFold(n_splits=10)\n",
    "\n",
    "    # Define the target variable 'OffTask' and features (all columns except 'OffTask')\n",
    "    X = dataset.drop(columns=['OffTask', 'Unique-id', 'namea'])  # Features\n",
    "    y = dataset['OffTask']  # Target variable\n",
    "    y = y.map({'N': 0, 'Y': 1})\n",
    "\n",
    "\n",
    "\n",
    "     # Create lists to store evaluation metrics for each fold\n",
    "    kappa_scores = []\n",
    "    roc_auc_scores = []\n",
    "\n",
    "\n",
    "# # Define parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.1, 0.2], \n",
    "#     'n_estimators': [50, 100, 200], \n",
    "#     'max_depth': [5, 6, 7],  \n",
    "#     'alpha': [0.1, 1, 10],\n",
    "#     'lambda': [0.1, 1, 10]\n",
    "# }\n",
    "\n",
    "# # Initialize classifier\n",
    "# clf = xgb.XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='error',\n",
    "#     use_label_encoder=False,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Define custom scorer\n",
    "# kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "# # Initialize GridSearchCV with custom scorer\n",
    "# grid_clf = GridSearchCV(clf, param_grid, scoring=kappa_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# # Fit model\n",
    "# grid_clf.fit(X, y)  # Ensure your feature matrix X and target vector y are appropriately defined\n",
    "\n",
    "# # Get best parameters\n",
    "# best_params = grid_clf.best_params_\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Initialize the parameters for the XGBClassifier\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'error',\n",
    "        'use_label_encoder': False,\n",
    "        'learning_rate': 0.2,  # Reduced learning rate\n",
    "        'n_estimators': 100,  # Increase n_estimators if you decrease learning rate\n",
    "        'random_state': 10,\n",
    "        'max_depth': 5,  # Limiting max_depth. You may want to optimize this using cross-validation\n",
    "        'alpha': 0.1,  # L1 regularization\n",
    "        'lambda': 1,  # L2 regularization\n",
    "    }\n",
    "\n",
    "    #feature selection\n",
    "    # Create the XGBClassifier model\n",
    "    clf_whole_dataset = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Fit the classifier to the entire dataset for feature selection\n",
    "    clf_whole_dataset.fit(X, y)\n",
    "\n",
    "    # Get feature importances from the trained model\n",
    "    feature_importances = clf_whole_dataset.feature_importances_\n",
    "\n",
    "    # Sort features based on importance\n",
    "    sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # Choose the top N features (for example, select top 10 features)\n",
    "    top_n = 5\n",
    "    selected_features = X.columns[sorted_idx[:top_n]]\n",
    "\n",
    "    # Update your feature matrix X to include only selected features\n",
    "    X_selected = X[selected_features]\n",
    "\n",
    "\n",
    "    # Iterate through the folds using the reduced feature set\n",
    "    for train_index, test_index in group_kfold.split(X_selected, y, groups):\n",
    "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #  # Iterate through the folds using the reduced feature set\n",
    "    #     for train_index, test_index in group_kfold.split(X_selected, y, groups):\n",
    "    #         X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
    "    #         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Create a DecisionTreeClassifier with specified hyperparameters\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        # Use early stopping\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        clf.fit(X_train, y_train, eval_metric=\"error\", eval_set=eval_set, early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "        # Use early stopping\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        clf.fit(X_train, y_train, eval_metric=\"error\", eval_set=eval_set, early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "\n",
    "        # Fit the classifier to the training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate Cohen's Kappa\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        kappa_scores.append(kappa)\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "\n",
    "# Calculate the mean values of the evaluation metrics across all folds\n",
    "    mean_kappa_score = np.mean(kappa_scores)\n",
    "    mean_roc_auc = np.mean(roc_auc_scores)\n",
    "\n",
    "    return mean_kappa_score, mean_roc_auc\n",
    "\n",
    "# # print(\"Mean Cohen's Kappa Score:\", mean_kappa_score)\n",
    "# print(\"Mean ROC AUC:\", mean_roc_auc)\n",
    "# print(\"Mean kappa:\", mean_kappa_score)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the first dataset\n",
    "kappa_dataset1, roc_auc_dataset1 = evaluate_dataset('ca1-dataset_new.csv')\n",
    "\n",
    "# Evaluate the second dataset\n",
    "kappa_dataset2, roc_auc_dataset2 = evaluate_dataset('ca1-dataset.csv')\n",
    "\n",
    "# Print and compare the metrics\n",
    "print(f\"Cohen's Kappa for new Dataset: {kappa_dataset1}\")\n",
    "print(f\"Cohen's Kappa for CA1 Dataset: {kappa_dataset2}\")\n",
    "print(f\"ROC AUC for new Dataset: {roc_auc_dataset1}\")\n",
    "print(f\"ROC AUC for CA1 Dataset: {roc_auc_dataset2}\")\n",
    "\n",
    "# Compare Kappa\n",
    "if kappa_dataset1 > kappa_dataset2:\n",
    "    print(\"new Dataset has a higher Cohen's Kappa score.\")\n",
    "elif kappa_dataset1 < kappa_dataset2:\n",
    "    print(\"Dataset 2 has a higher Cohen's Kappa score.\")\n",
    "else:\n",
    "    print(\"Both datasets have the same Cohen's Kappa score.\")\n",
    "\n",
    "# Compare ROC AUC\n",
    "if roc_auc_dataset1 > roc_auc_dataset2:\n",
    "    print(\"new Dataset has a higher ROC AUC score.\")\n",
    "elif roc_auc_dataset1 < roc_auc_dataset2:\n",
    "    print(\"Dataset 2 has a higher ROC AUC score.\")\n",
    "else:\n",
    "    print(\"Both datasets have the same ROC AUC score.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3b0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f8037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
